# -*- coding: utf-8 -*-
"""IE_643_DATASET_CODE (1).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19zC9qQNr3igQDE71Z6AGKlzFjYO2_bpf

# Importing necessary libraries
"""

import bs4
import requests
import json
import re
import html
import pandas as pd

"""# Extracting the URLs"""

#code for extracting all the news urls from the money control website
urls = []

#URLs for news related to different stock prices and trends
for i in range(1,30):
  url = f"https://www.moneycontrol.com/news/business/stocks/page-{i}/"
  request = requests.get(url)
  soup = bs4.BeautifulSoup(request.text, 'html.parser')

  ul_cagetory = soup.find('ul', id='cagetory')

  # Extract all URLs within <a> tags inside <h2> tags within this <ul>
  urls_per_page = [a['href'] for h2 in ul_cagetory.find_all('h2') for a in h2.find_all('a', href=True)]
  urls.extend(urls_per_page)

filtered_urls = [url for url in urls if '/live-' not in url]

len(filtered_urls)

"""# Scraping Data from Individual URLs"""

#function for extracting the text body from a URL in moneycontrol
def extract_body(url) :
    request = requests.get(url)
    soup = bs4.BeautifulSoup(request.text, 'html.parser')
    all_script = soup.find_all('script', attrs = {'type' : 'application/ld+json'})
    raw_article_str = all_script[3].get_text().replace('\r\n', ' ')
    parts = re.split(r"""("[^"]*"|'[^']*')""", raw_article_str)
    parts[::2] = map(lambda s: "".join(s.split()), parts[::2])
    article_str = "".join(parts)
    article_str = article_str[1:]
    article_str = article_str[:-1]
    article_dict = json.loads(article_str)
    all_tags = soup.find_all('div', attrs = {'class': 'tags_first_line'})
    lst_all_tags = []
    for i in all_tags:
      lst_all_tags.append(i.text)
    tags = lst_all_tags[0].replace('Tags:', ' ')
    tags = tags.split('#')
    tags = tags[1:]
    tags = ', '.join([str(elem).strip() for elem in tags])
    article_dict['tags'] = tags
    return article_dict['articleBody']

#creating a dictionary to store the data obtained from scaping the URLs
dict = {
    'url' : [],
    'title' : [],
    'summary' : [],
    'body' : []
}

#code for looping through all the URLs and extracting the data from them
iter = 0
for url in filtered_urls:
  request = requests.get(url)
  soup = bs4.BeautifulSoup(request.text, 'html.parser')
  print(url)
  print(iter)
  try :
    dict['body'].append(extract_body(url))
    dict['url'].append(url)
    dict['title'].append(soup.title.text)
    h2_article = soup.find('h2', class_='article_desc')
    if h2_article:
      text_content = h2_article.get_text(strip=True)
      dict['summary'].append(text_content)
    else:
      dict['summary'].append(None)
  except (IndexError, json.JSONDecodeError) as e:
    print(f"Skipping URL due to {type(e).__name__}:{url}")
    continue
  iter += 1

"""# Data Cleaning"""

#function for cleaning the raw text obtained by scraping the moneycontrol URLs
def clean_data(raw_text):
  # Step 1: Convert HTML entities to text
  clean_text = html.unescape(raw_text)

# Step 2: Remove specific "Also read:" phrases without removing the rest of the content
# Modify to match only "Also read:" and the immediate phrase or link after it.
  clean_text = re.sub(r'Also read:\s?&nbsp;[^.]+\.?', '', clean_text)

# Step 3: Remove "Disclaimer" sections
  clean_text = re.sub(r'Disclaimer:.+?(?=\n|$)', '', clean_text)

# Step 4: Remove any remaining unwanted HTML tags or symbols
  clean_text = re.sub(r'<[^>]+>', '', clean_text)  # Removes any HTML tags if present

# Step 5: Strip any remaining extra whitespace or new lines
  clean_text = re.sub(r'\s+', ' ', clean_text).strip()
  return clean_text

#using the cleaning function to clean the raw textual data
df['body'] = df['body'].apply(clean_data)

#converting the dictionary into a pandas dataframe.
df = pd.DataFrame(dict)
df

"""# Storing Clean Data"""

#storing the data into a csv file
df.to_csv('data.csv', index=False)

"""# Data Pre-processing"""

#converting the data into the proper format to feed into the model for fine-tuning

# Load your CSV file (adjust path as needed)
file_path = '/content/data (3).csv'
df = pd.read_csv(file_path)

# Define output formats for both datasets
formatted_data_body = ""
formatted_data_summary_url = ""

# Generate formatted data for both datasets
for index, row in df.iterrows():
    title = row['title']
    summary = row['summary']
    body = row['body']
    url = row['url']

    # Dataset 1: Title -> Body
    question_body = f"<s>[INST] Describe the details of the news titled: {title} [/INST]"
    answer_body = f"The full article is as follows: {body}."
    entry_body = f"{question_body} {answer_body} </s>\n"
    formatted_data_body += entry_body

    # Dataset 2: Title -> Summary and URL
    question_summary_url = f"<s>[INST] Provide a summary and source for the news titled: {title} [/INST]"
    answer_summary_url = f"The summary is: {summary}. Source: {url}."
    entry_summary_url = f"{question_summary_url} {answer_summary_url} </s>\n"
    formatted_data_summary_url += entry_summary_url

# Save datasets to files
output_file_body = 'formatted_news_data_body.txt'
output_file_summary_url = 'formatted_news_data_summary_url.txt'

with open(output_file_body, 'w') as f_body:
    f_body.write(formatted_data_body)

with open(output_file_summary_url, 'w') as f_summary_url:
    f_summary_url.write(formatted_data_summary_url)

print(f"Formatted data saved to '{output_file_body}' and '{output_file_summary_url}' on your local device.")

"""# Model Fine-Tuning

"""

!pip install -q accelerate==0.21.0 peft==0.4.0 bitsandbytes==0.40.2 transformers==4.31.0 trl==0.4.7

from google.colab import drive
drive.mount('/content/drive')

import os
import torch
from datasets import load_dataset
from datasets import Dataset
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    BitsAndBytesConfig,
    HfArgumentParser,
    TrainingArguments,
    pipeline,
    logging,
)
from peft import LoraConfig, PeftModel
from trl import SFTTrainer

# The model that you want to train from the Hugging Face hub
model_name = "NousResearch/Llama-2-7b-chat-hf"

# The instruction dataset to use
file_path = "/content/formatted_news_data_body.txt"
with open(file_path, "r") as file:
    lines = file.readlines()

# Each line is already in the expected format for LLaMA-2, so we can wrap it in a dictionary
data = [{"text": line.strip()} for line in lines]

# Convert to Hugging Face Dataset
dataset = Dataset.from_list(data)

# Fine-tuned model name
new_model = "Llama-2-7b-chat-finetune"

################################################################################
# QLoRA parameters
################################################################################

# LoRA attention dimension
lora_r = 64

# Alpha parameter for LoRA scaling
lora_alpha = 16

# Dropout probability for LoRA layers
lora_dropout = 0.1

################################################################################
# bitsandbytes parameters
################################################################################

# Activate 4-bit precision base model loading
use_4bit = True

# Compute dtype for 4-bit base models
bnb_4bit_compute_dtype = "float16"

# Quantization type (fp4 or nf4)
bnb_4bit_quant_type = "nf4"

# Activate nested quantization for 4-bit base models (double quantization)
use_nested_quant = False

################################################################################
# TrainingArguments parameters
################################################################################

# Output directory where the model predictions and checkpoints will be stored
output_dir = "./results"

# Number of training epochs
num_train_epochs = 1

# Enable fp16/bf16 training (set bf16 to True with an A100)
fp16 = False
bf16 = False

# Batch size per GPU for training
per_device_train_batch_size = 4

# Batch size per GPU for evaluation
per_device_eval_batch_size = 4

# Number of update steps to accumulate the gradients for
gradient_accumulation_steps = 1

# Enable gradient checkpointing
gradient_checkpointing = True

# Maximum gradient normal (gradient clipping)
max_grad_norm = 0.3

# Initial learning rate (AdamW optimizer)
learning_rate = 2e-4

# Weight decay to apply to all layers except bias/LayerNorm weights
weight_decay = 0.001

# Optimizer to use
optim = "paged_adamw_32bit"

# Learning rate schedule
lr_scheduler_type = "cosine"

# Number of training steps (overrides num_train_epochs)
max_steps = -1

# Ratio of steps for a linear warmup (from 0 to learning rate)
warmup_ratio = 0.03

# Group sequences into batches with same length
# Saves memory and speeds up training considerably
group_by_length = True

# Save checkpoint every X updates steps
save_steps = 0

# Log every X updates steps
logging_steps = 25

################################################################################
# SFT parameters
################################################################################

# Maximum sequence length to use
max_seq_length = None

# Pack multiple short examples in the same input sequence to increase efficiency
packing = False

# Load the entire model on the GPU 0
device_map = {"": 0}

# Load tokenizer and model with QLoRA configuration
compute_dtype = getattr(torch, bnb_4bit_compute_dtype)

bnb_config = BitsAndBytesConfig(
    load_in_4bit=use_4bit,
    bnb_4bit_quant_type=bnb_4bit_quant_type,
    bnb_4bit_compute_dtype=compute_dtype,
    bnb_4bit_use_double_quant=use_nested_quant,
)

# Check GPU compatibility with bfloat16
if compute_dtype == torch.float16 and use_4bit:
    major, _ = torch.cuda.get_device_capability()
    if major >= 8:
        print("=" * 80)
        print("Your GPU supports bfloat16: accelerate training with bf16=True")
        print("=" * 80)

# Load base model
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    quantization_config=bnb_config,
    device_map=device_map
)
model.config.use_cache = False
model.config.pretraining_tp = 1

# Load LLaMA tokenizer
tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = "right" # Fix weird overflow issue with fp16 training

# Load LoRA configuration
peft_config = LoraConfig(
    lora_alpha=lora_alpha,
    lora_dropout=lora_dropout,
    r=lora_r,
    bias="none",
    task_type="CAUSAL_LM",
)

# Set training parameters
training_arguments = TrainingArguments(
    output_dir=output_dir,
    num_train_epochs=num_train_epochs,
    per_device_train_batch_size=per_device_train_batch_size,
    gradient_accumulation_steps=gradient_accumulation_steps,
    optim=optim,
    save_steps=save_steps,
    logging_steps=logging_steps,
    learning_rate=learning_rate,
    weight_decay=weight_decay,
    fp16=fp16,
    bf16=bf16,
    max_grad_norm=max_grad_norm,
    max_steps=max_steps,
    warmup_ratio=warmup_ratio,
    group_by_length=group_by_length,
    lr_scheduler_type=lr_scheduler_type,
    report_to="tensorboard"
)

# Set supervised fine-tuning parameters
trainer = SFTTrainer(
    model=model,
    train_dataset=dataset,
    peft_config=peft_config,
    dataset_text_field="text",
    max_seq_length=max_seq_length,
    tokenizer=tokenizer,
    args=training_arguments,
    packing=packing,
)

# Train model
trainer.train()

# Save trained model
trainer.model.save_pretrained(new_model)

# Commented out IPython magic to ensure Python compatibility.
# %load_ext tensorboard
# %tensorboard --logdir results/runs

"""During each forward pass, the model processes a batch of input data from the training dataset, which includes tokenized sequences of text. In this step:

The model generates logits—raw, unnormalized prediction scores—for each token in the sequence. These logits are the model’s predictions before any activation or probability adjustments.

Next, the logits are compared with the true tokens to calculate the loss. In causal language modeling, this is typically done using cross-entropy loss, a function that measures the difference between the predicted probability distribution and the true distribution (i.e., the actual sequence of tokens).

The cross-entropy loss is calculated by transforming logits into probabilities using a softmax function. It then measures the error between these predicted probabilities and the actual tokens in the sequence. This loss is averaged across all tokens in the batch to provide a single measure of error for that batch.

Once the loss is computed, backpropagation occurs. This process calculates the gradients for each parameter in the model, indicating the direction and magnitude by which each parameter should be adjusted to minimize the loss.

Using these gradients, an optimizer updates the model parameters, moving them in the direction that reduces the overall loss. For models that support gradient accumulation, this update can occur over multiple steps to effectively increase the batch size when GPU memory is limited.

Throughout the training process, the trainer logs the loss periodically, allowing for insights into model performance and convergence over time. This loss metric provides a feedback loop to ensure the model is learning effectively.

This structured training process—forward pass, loss calculation, backpropagation, and parameter updates—is repeated until the model reaches a minimum loss or the specified number of training epochs, effectively fine-tuning the model to the task at hand.

# Code for Interface
"""

!pip install gradio
import gradio as gr
from transformers import pipeline

# Set up the pipeline for text generation
pipe = pipeline(task="text-generation", model=model, tokenizer=tokenizer, max_length=200)

# Initialize a list to store ratings
ratings = []

# Define the function that generates responses
def generate_response(prompt):
    result = pipe(prompt)  # Directly use the prompt without extra markers
    generated_text = result[0]['generated_text'].strip()  # Return the generated text
    return generated_text

# Define the function to rate the response and calculate the average rating
def rate_response(rating):
    ratings.append(rating)
    avg_rating = sum(ratings) / len(ratings) if ratings else 0
    return f"Average Rating: {avg_rating:.2f}"

# Define a combined function to handle both response generation and rating
def response_and_rating(prompt, rating):
    response = generate_response(prompt)
    rating_message = rate_response(rating)
    return response, rating_message

# Create the Gradio interface for generating responses and rating
interface = gr.Interface(
    fn=response_and_rating,
    inputs=[
        gr.Textbox(label="Enter your question or prompt:"),
        gr.Slider(minimum=0, maximum=10, step=1, value=10, label="Rate the Response (0 to 10)")
    ],
    outputs=[
        gr.Textbox(label="Generated Response"),
        gr.Textbox(label="Average Rating")
    ],
    title="Chatbot Interface",
    description="Ask questions to your chatbot model and rate the response."
)

# Launch the interface
interface.launch(share=True)

"""# Accuracy Testing"""

from transformers import pipeline
from nltk.translate.bleu_score import sentence_bleu

# Set up the pipeline for text generation
pipe = pipeline(task="text-generation", model=model, tokenizer=tokenizer, max_length=200)

# Define a function to generate a response and calculate the BLEU score
def generate_and_calculate_bleu(prompt, reference):
    # Generate a response from the model
    result = pipe(f"<s>[INST] {prompt} [/INST]")
    generated_text = result[0]['generated_text']

    # Calculate BLEU score
    reference_tokens = reference.split()
    generated_tokens = generated_text.split()
    bleu_score = sentence_bleu([reference_tokens], generated_tokens)

    return generated_text, bleu_score

# Example usage
prompt = "Provide a summary and source for the news titled: Taking Stock: Nifty closes above 24,450, Sensex up 364 points; mid-, small-caps shine"
reference = "The full article is as follows: Geojit Financial Services research report on Zomato In Q2FY25, revenue surged 68.5% YoY to reach Rs 4,799cr, driven by strong performance in key segments. The India food ordering and delivery segment witnessed a 30.1% YoY increase in revenue to Rs 2,012cr, whereas Hyperpure and quick-commerce businesses saw an increase of 97.7% and 128.9% YoY, respectively. EBITDA stood at Rs 226cr, largely driven by platform fees and higher ad revenue, and might only witness a stable rise in the near term. The EBITDA margin increased to 4.7% from 1.7% in Q2FY24, primarily driven by revenue increases, partly offset by elevated operating expenses, which included wage increases and expenditures associated with the expansion of BlinkIt. The expansion of the food delivery sector is likely to be driven by an increase in user acquisition and order frequency, while the average order value (AOV) is expected to experience modest annual growth in low single digits amid a challenging demand situation in the quick service restaurant (QSR) industry. Outlook Hence, we reiterate our BUY rating on the stock with a revised target price of Rs 284 based on 8.5x FY26E price/ sales. For all recommendations report, click here."

# Generate response and calculate BLEU score
generated_text, bleu_score = generate_and_calculate_bleu(prompt, reference)

# Output the results
print("Generated Text:", generated_text)
print("BLEU Score:", bleu_score)